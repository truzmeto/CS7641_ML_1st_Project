---
title: "CS7641 Machine Learning First Assignment "
author: "T. Ruzmetov"
date: "September 20, 2017"
output: 
  pdf_document
---

\fontfamily{cmr}
\fontsize{12}{22}
\selectfont

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(warn=-1)
```


#  Introduction

Purpose of this assignment is to investigate and compare the performance of five different machine
learning techniques such as K-Nearest Neighbor, Decision Trees with pruning, Boosted Derision Trees, Support Vector
Machines and Artificial Neural Networks by applying them to two distinct classification problems(distinct datasets).
In order to fulfill the requirement, "Lending Club Data set" from Kaggle and Adult Data set from UCI machine learning
repository are used. For all methods on both data sets cross validation is performed in order to find optimum hyper
parameter. Learning curve experiment is done by altering training data size and monitoring performance via prediction accuracy.    


## Lending Club Dataset

Lending Club is the world’s largest online marketplace connecting borrowers and investors. Getting loan has become
common practice in developed countries, which makes it interesting to learn how banks determine customer eligibility
or credit worthiness given some information. In this problem, we apply unsupervised learning methods to predict if
customer is going to pay or default on their loan based on provided set of features with labels. Provided data set
contain complete loan data for all loans issued through 2007 to 2015, including the current loan status(Current,
Late, Fully Paid, etc.). Instances with "Current" loan status are discarded to make it simple binary classification
problem with two labels(unpaid,paid). After cleaning and feature extraction, the data set contains 15 features and
285373 samples. 90% of entire data is allocated to training set and 10% is separated out for testing. Then only only
20% of the training set is used for model training due to large size resulting in high time consumption.  

```{r, echo=FALSE}
# put targe distributions
```

## Adult Dataset

The adult data set contains census information from 1994.  Our task is to predict whether a person makes more
than $50K/year. After preprocessing is applied, there are 11 features and 45000 remaining instances. The target 
feature "income" is labeled as "high" when income is greater than $50K and "low" when it is less. 


## Methods and Tools

R-Studio and R are used for both coding and project writing. R-studio has natively built in markdown + latex + html
via pandoc, that I used for project writing. Caret package in R provides really nicely build in libraries for a
lot of machine learning techniques and very user friendly inplementation of parallel computing for cross validation
via hyper parameter tuning. For all ML methods, I used 5-fold cross validation with repetations(2-5) in order to find
parameters that correspond to maximum cross validation accuracy. For growing and pruning the tree "rpart" package is
used.


## K-Nearest Naighbour 

Knn algorithm classifies a data points based on its K closest neighbors in distance, where over represented
class within K will get the vote. K value and distance metric are the only parameters to tune for cross validation.
It is slow with large data sets and high dimensional data. Also categorical features don't work well. 5-fold twice
repeated cross validation by changing k value from 5 to 43 with step size 2 gave $k_{best}=43$ for Lending Club data
set and $k_{best}=27$ for Adult Data set as depicted in Fig.\ref{fig:knn_val}. This values of k correspond to maximum
accuracy. Then, using the best model estimated accuracies on test set are 73.1% for LC data and 83.1% for Adult Data.  


\begin{figure}
  \center
    \includegraphics[width=6.0cm]{figs/LC_knn_acc_naigh.pdf}
    \includegraphics[width=6.0cm]{figs/AD_knn_acc_naigh.pdf}
  \center
  \caption{Cross validation plots for KNN model}
  \label{fig:knn_val}
\end{figure}


Although Adult Data set contain mostly categorical features it performed better than Lending Club Data set.
Some features such "country", "education number" and "fnlwgt" are removed! 



```{r,echo=FALSE}
library(knitr)
LC_con_mat_knn <- read.table("output/LC_confusion_mat_knn.txt", sep = "", header = TRUE)
AD_con_mat_knn <- read.table("output/AD_confusion_mat_knn.txt", sep = "", header = TRUE,
                             col.names = c("high","low"),row.names = c("high","low"))
AD_con_mat_knn$row.names <- NULL
kable(list(LC_con_mat_knn, AD_con_mat_knn),caption = "KNN confusion matrix for Lending
      Club(left) and Adult data(right)")
```


\begin{figure}
  \center
    \includegraphics[width=8.0cm]{figs/LC_knn_learning_curve.png}
    \includegraphics[width=8.0cm]{figs/AD_knn_learning_curve.png}
  \center
  \caption{Learning curve plots for KNN model}
  \label{fig:lear_curve_knn}
\end{figure}

Learning curve results for LC Data set Fig.\ref{fig:lear_curve_knn} show decrease in training accuracy as
training size grow, but testing accuracy don't change much after it reaches 20000 sample size. On the other hand, 
for Adult Data both training and testing accuracies increase with data size meaning on could add more data to get
better accuracy on unseen data. 



## Decision Tree Algorithm with Pruning

Decision tree classifier is used with maximum information gain to determine wich feature is given a priority for
splitting. Post pruning is used to prevent overfitting. Tree is grown to a large size (1000-6200) and complexity
parameter with least x-value error is choosen with corresponding optimum size of tree to proceed further. Number
of splits(tree size) is reduced to from 6200 to 29(cp=0.00059) for Lending Club data and from 2284 to 82(cp=0.00044)
for Adult Data respectivaly.

\begin{figure}
  \center
    \includegraphics[width=17.0cm]{figs/LC_pruned_tree_diag.png}
  \center
  \label{fig:LC_pruned_tree}
\end{figure}


```{r,echo=FALSE}
LC_tree_tab <- read.table("output/LC_tree_pre_post_pruning_results.txt", sep = "", header = TRUE)
AD_tree_tab <- read.table("output/AD_tree_pre_post_pruning_results.txt", sep = "", header = TRUE)
LC_tree_tab <- LC_tree_tab[c("Accuracy","Kappa")]
AD_tree_tab <- AD_tree_tab[c("Accuracy","Kappa")]
  
kable(list(LC_tree_tab, AD_tree_tab),caption = "Decision Trees pre and post pruning performance tabulated for Lending
      Club(left) and Adult data(right)")
```


For both data sets accuracy of training set reduced due to pruning, while accuracy of testing set is improoved as
expected. Effect of pruning on test set is more pronounced for Lending Club Data set showing around ~8% inclrease,
while Adult Data set show ~3% improovement. Pre and post pruning results are summarized in table2 via accuracy and
kappa value metric.


\begin{figure}
  \center
    \includegraphics[width=8.0cm]{figs/LC_tree_cp_xerror.png}
    \includegraphics[width=8.0cm]{figs/AD_tree_cp_xerror.png}
  \center
  \caption{Cross validation plots for Tree model}
  \label{fig:cross_val_tree}
\end{figure}

Learinig curve results for decision tree method for both data sets form a plateu at about 70% of training data size for Lending
Club Data and at around 80% of training size of Adult Data set. This suggests that both data sets have more than enough amount of
training data for model to converge to optimum performance. For learning curve iterations by changing training set size I performed
post pruning for every iteration. For that reason a learning curve for Lending Lclub Data set looks different than all other learning
curves done by different models. More specifically rest of the models have a gap between training and testing error after convergence except decision tree model.


\begin{figure}
  \center
    \includegraphics[width=8.0cm]{figs/LC_tree_learning_curve.png}
    \includegraphics[width=8.0cm]{figs/AD_tree_learning_curve.png}
  \center
  \caption{Learning curve plots for tree model}
  \label{fig:lear_curve_tree}
\end{figure}


```{r,echo=FALSE}
LC_con_mat_tree <- read.table("output/LC_confusion_mat_tree.txt", sep = "", header = TRUE)
AD_con_mat_tree <- read.table("output/AD_confusion_mat_tree.txt", sep = "", header = TRUE)
kable(list(LC_con_mat_tree, AD_con_mat_tree),caption = "Pruned Decision Trees confusion matrix for Lending
      Club(left) and Adult data(right)")
```


## Gradient Boosting

For Decision Tree Boosting the 'gbm' method is used under caret package. It implements extensions to Freund and Schapire’s
AdaBoost algorithm and Friedman’s gradient boosting machine. Repeated cross validation is performed by tuning complexity of
the tree(interaction.depth) and learning rate(shrinkage) for 50 terations with stepsize 2. All values for above mentioned
hyper parameters are shown in Fig.\ref{fig:cross_val_boost}. Optimum values choosen for Lending Club Data n.trees = 100,
interaction.depth=9, shrinkage=0.15 ,and for Adult Data: n.trees = 80, interaction.depth=5, shrinkage=0.15. The minimum number
of of training set sampleas in node to commence splitting is kept constant at 20. 



```{r,echo=FALSE,eval=FALSE}
## GBM - gradient boosting machine

# parameters:
# number of iterations, i.e. trees, (called n.trees in the gbm function)
# complexity of the tree, called interaction.depth
# learning rate: how quickly the algorithm adapts, called shrinkage
# the minimum number of training set samples in a node to commence splitting (n.minobsinnode)
```


\begin{figure}
  \center
    \includegraphics[width=8.0cm]{figs/LC_boost_acc_iter_shrink.pdf}
    \includegraphics[width=8.0cm]{figs/AD_boost_acc_iter_shrink.pdf}
  \center
  \caption{Cross validation plots for boosted tree}
  \label{fig:cross_val_boost}
\end{figure}

Learning curves for Lendig Club Data set show
that both training and testing accuracies converge at training size 20000 having big gap in between. It is likely due to model
suffering from overfitting. Also for this data set we can see that cross validation curves did not yet reach plateu. I think
adding more iterations could result in better rperformance. On the other hand, Adult Data set performed very good acheiving ~85%
accuracy over testing data set. For a given number of iterations(n.trees) cross validation curves seem to have converged. Learning
curves converge quickly as training size grow having almost no gap in between training an testing curves. This suggests that there
is a good balance between bias and variance. 

\begin{figure}
  \center
    \includegraphics[width=8.0cm]{figs/LC_boosting_learning_curve.png}
    \includegraphics[width=8.0cm]{figs/AD_boosting_learning_curve.png}
  \center
  \caption{Learning curve plots for tree model}
  \label{fig:lear_curve_boost}
\end{figure}


```{r,echo=FALSE}
LC_con_mat_boost <- read.table("output/LC_confusion_mat_boost.txt", sep = "", header = TRUE)
AD_con_mat_boost <- read.table("output/AD_confusion_mat_boost.txt", sep = "", header = TRUE)

kable(list(LC_con_mat_boost, AD_con_mat_boost),caption = "Boosting confusion matrix for Lending
      Club(left) and Adult Data(right)")
```

\pagebreak

## Neural Networks

"nnet" function is used through "caret" package in order to construct simple neural network model with single
hidden layer of sigmoid activation function neurons. It uses backpropagation algoritm to minimize training error.
Optimization parameters are number of hidden units and weight decay. Parameter set choosen for both data sets
are displayed in Fig.\ref{fig:cross_val_nnet}. Best parameters for Lending Club Data set are n.hidden.unit=7,
weight.decay=0.08, where for Adult Data set n.hidden.units=5, weight.decay=0.01. Cross validation curves represented via
ROC type accuracy show that there is a room for further improovement for Lending Club Data set because for all values of
weight decay parameter ROC value is making upward progress as we increase number of hidden units. In contrast, plot
shown for Adult Data set have very close ROC value for hidden unit number 4 and 5 which gives more reliable optimum parameters.

\begin{figure}
  \center
    \includegraphics[width=8.0cm]{figs/LC_nnet_ROC_units_weight.pdf}
    \includegraphics[width=8.0cm]{figs/AD_nnet_ROC_units_weight.pdf}
  \center
  \caption{Cross validation plots for NNet}
  \label{fig:cross_val_nnet}
\end{figure}

Again Adalt Data winning over the Lending Club Data set by illustrating 10% better accuracy on testing set and %7 on training set.
Training and testing accuracies for both data sets reach plateu fast suggesting 10000 data points could be sufficient to train.

\begin{figure}
  \center
    \includegraphics[width=8.0cm]{figs/LC_nnet_learning_curve.png}
    \includegraphics[width=8.0cm]{figs/AD_nnet_learning_curve.png}
  \center
  \caption{Learning curve plots for tree model}
  \label{fig:lear_curve_nnet}
\end{figure}


```{r,echo=FALSE}
LC_con_mat_nnet <- read.table("output/LC_confusion_mat_nnet.txt", sep = "", header = TRUE)
AD_con_mat_nnet <- read.table("output/AD_confusion_mat_nnet.txt", sep = "", header = TRUE)

kable(list(LC_con_mat_boost, AD_con_mat_boost),caption = "NNet confusion matrix for Lending
      Club(left) and Adult Data(right)")
```


## Support Vector Machines

Suppot Vector Machine is unsupervised learning algorithm mostly used for classification problems.
It classifies instances by choosing an optimum decision boundary(can be both lenear and non-linear)
with maximum margin, where length of the margin is half distance between two support vectors. Here
I applied SVM classifier for both data sets using two differenrr kernels: Linear and Radial. Tried
polynomial one too, but since training was very time consuming decided to drop that one. For each
choice of kernel, I performed 5 fold cross validation, Fig.\ref{fig:cross_val_svm}, to choose optimum
hyper parameters. On Lending Club Data set radial kernel(acc=73.4%) performed a little better than
linear kernel(72.8%), but performance of different kernels on Adult Data set was very close. So I decided
to go further with radial kernel Fig.\ref{fig:mod_comp_svm}. 



```{r,echo=FALSE,eval=FALSE}
#LC_ac_svm_rad = 73.4%
#LC_ac_svm_lin = 72.8%
#AD_ac_svm_rad = 84.4%
#AD_ac_svm_lin = 84.4% 


#  Pros:
#        It works really well with clear margin of separation
#        It is effective in high dimensional spaces.
#        It is effective in cases where number of dimensions is greater than the number of samples.
#        It uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.
#  Cons:
#        It doesn’t perform well, when we have large data set because the required training time is higher
#        It also doesn’t perform very well, when the data set has more noise i.e. target classes are overlapping
#        SVM doesn’t directly provide probability estimates, these are calculated using an expensive five-fold cross-validation. 

```


\begin{figure}
  \center
    \includegraphics[width=8.0cm]{figs/LC_svm_acc_cost_sigmaRad.pdf}
    \includegraphics[width=8.0cm]{figs/AD_svm_acc_cost_sigma.pdf}
  \center
  \caption{Cross validation plots for SVM}
  \label{fig:cross_val_svm}
\end{figure}

A surprizing result I detected with SVM radial
kernel classifier is that learning curves Fig.\ref{fig:lear_curve_svm} for training and test sets in Adult
Data set did not even come close. They both seem to have reached the plateu, but there is a gap, which is
different than behaviour of all other models.  


```{r,echo=FALSE}
LC_con_mat_svm <- read.table("output/LC_confusion_mat_svmLin.txt", sep = "", header = TRUE)
AD_con_mat_svm <- read.table("output/AD_confusion_mat_svmLin.txt", sep = "", header = TRUE)

kable(list(LC_con_mat_svm, AD_con_mat_svm),caption = "SVM RBB confusion matrix for Lending
      Club(left) and Adult Data(right)")
```



\begin{figure}
  \center
    \includegraphics[width=8.0cm]{figs/LC_svm_learning_curve.png}
    \includegraphics[width=8.0cm]{figs/AD_svm_learning_curveRad.png}
  \center
  \caption{Learning curve plots for SVM model for Lending
      Club(left) and Adult Data(right)}
  \label{fig:lear_curve_svm}
\end{figure}



\begin{figure}
  \center
    \includegraphics[width=7.0cm]{figs/LC_svm_model_compare.pdf}
    \includegraphics[width=7.0cm]{figs/AD_svm_model_compare.pdf}
  \center
  \caption{Comparison between linear and radial Kernels for SVM model for Lending
      Club(left) and Adult Data(right)}
  \label{fig:mod_comp_svm}
\end{figure}


```{r,echo=FALSE}

LC_time_svm <- read.table("output/LC_learning_results_svm.txt", sep = "", header = TRUE)
LC_time_svm$model <- "svm"
LC_time_svm <- LC_time_svm[-1,]
LC_time_knn <- read.table("output/LC_learning_results_knn.txt", sep = "", header = TRUE)
LC_time_knn$model <- "knn"
LC_time_tree <- read.table("output/LC_learning_results_tree.txt", sep = "", header = TRUE)
LC_time_tree$model <- "tree"
LC_time_boost <- read.table("output/LC_learning_results_boost.txt", sep = "", header = TRUE)
LC_time_boost$model <- "boost"
LC_time_nnet <- read.table("output/LC_learning_results_nnet.txt", sep = "", header = TRUE)
LC_time_nnet$model <- "nnet"
LC_time_all <- rbind(LC_time_svm,LC_time_knn,LC_time_tree,LC_time_boost,LC_time_nnet)

AD_time_svm <- read.table("output/AD_learning_results_svm.txt", sep = "", header = TRUE)
AD_time_svm$model <- "svm"
AD_time_svm <- AD_time_svm[-1,]
AD_time_knn <- read.table("output/AD_learning_results_knn.txt", sep = "", header = TRUE)
AD_time_knn$model <- "knn"
AD_time_tree <- read.table("output/AD_learning_results_tree.txt", sep = "", header = TRUE)
AD_time_tree$model <- "tree"
AD_time_boost <- read.table("output/AD_learning_results_boost.txt", sep = "", header = TRUE)
AD_time_boost$model <- "boost"
AD_time_nnet <- read.table("output/AD_learning_results_nnet.txt", sep = "", header = TRUE)
AD_time_nnet$model <- "nnet"
AD_time_all <- rbind(AD_time_svm,AD_time_knn,AD_time_tree,AD_time_boost,AD_time_nnet)


# extract accuracies for each model
LC_accs <- aggregate(x=LC_time_all$test_accur, by = list(LC_time_all$model), max)
AD_accs <- aggregate(x=AD_time_all$test_accur, by = list(AD_time_all$model), max)

tab <- data.frame(LC_accs,AD_accs$x)
names(tab) <- c("model","Accuracy LC","Accuracy AD")
kable(tab,caption = " Accuracy Report Across Models and Data Sets")


```


```{r,echo=FALSE,fig.height=4,fig.width=8}
library("lattice")
library("plyr")
library("ggplot2")
library("Rmisc")
p1 <- ggplot(LC_time_all, aes(x=data_size, y=cpu_time, colour = model)) +
  geom_line() + 
  geom_point() +
  theme_bw() +
  labs(title = "Training size vs trainig time LC", x = "Training Size", y = "CPU Time", color="") +
  theme(legend.position = c(0.6,0.6),
        axis.title = element_text(size = 16.0),
        axis.text = element_text(size=10, face = "bold"),
        plot.title = element_text(size = 15, hjust = 0.5),
        #text = element_text(family="Times New Roman"),
        axis.text.x = element_text(colour="black"),
        axis.text.y = element_text(colour="black"))

p2 <- ggplot(AD_time_all, aes(x=data_size, y=cpu_time, colour = model)) +
  geom_line() + 
  geom_point() +
  theme_bw() +
  labs(title = "Training size vs trainig time AD", x = "Training Size", y = "CPU Time", color="") +
  theme(legend.position = c(0.2,0.6),
        axis.title = element_text(size = 16.0),
        axis.text = element_text(size=10, face = "bold"),
        plot.title = element_text(size = 15, hjust = 0.5),
        #text = element_text(family="Times New Roman"),
        axis.text.x = element_text(colour="black"),
        axis.text.y = element_text(colour="black"))

multiplot(p1,p2,cols=2)

```

\pagebreak

## Conclusion

Gradiet Boosting was the best model compared to all other algoithms for both data sets reaching the
accuracy of 74.2% for Lending Club Data set and 85.3% for Adult Data set. This is actually not so
surprising because combination of many weak learners should naturally result in better prediction for
specific type of data sets. Looks like both data sets have this property. Pruned decision tree performed as 
second best (LC.acc=73.8%, Ad.acc=84.6%). Lending Club Data did not perform as good as Adult Data set
supposedly due to noise, and it needs more and carefull feature engineering. Training data size vs training
clock time plots show that computation time is linear with data size except SVM model, which showed exponential
increase in cpu time.

We learn that the supervised learning methods
work well for data sets that have an underlining distribution, but when we
compare and contrast models, the differences in their performance can be
partly attributable not to their differing structure, but to the different levels
of tuning effort we invest in them. Also the types of algorithms that work
best are problem-specific.





